{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创造训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "data/tokens.train/tokens.08.txt\ndata/tokens.train/tokens.05.txt\ndata/tokens.train/tokens.04.txt\ndata/tokens.train/tokens.06.txt\ndata/tokens.train/tokens.07.txt\n"
    }
   ],
   "source": [
    "# 聚合所有数据\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for in_file in glob.glob(\"data/tokens.train/*.txt\"):\n",
    "    print(in_file)\n",
    "    for line in open(in_file):\n",
    "        label, sentense = line.strip().split(\"\\t\")\n",
    "        if label == \"0\"2\n",
    "            label = \"FF\"\n",
    "        elif label == \"1\":\n",
    "            label = \"MP\"\n",
    "        else:\n",
    "            assert \"Error\"\n",
    "        X.append(sentense)\n",
    "        y.append(label)\n",
    "\n",
    "data = {}\n",
    "data[\"label\"] = y\n",
    "data[\"text\"] = X\n",
    "pd.DataFrame(data).to_csv(\"data/train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "TEXT = torchtext.data.Field()\n",
    "LABEL = torchtext.data.LabelField()\n",
    "\n",
    "my_data = torchtext.data.TabularDataset(path='data/train.csv',\n",
    "                                        format='csv', \n",
    "                                        fields=[('labels', LABEL), ('text', TEXT)])\n",
    "\n",
    "# train_data, valid_data = my_data.split(split_ratio=0.8)\n",
    "# train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(my_data, min_freq=10)\n",
    "LABEL.build_vocab(my_data, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21142, 2)"
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "len(TEXT.vocab), len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "defaultdict(None, {'MP': 0, 'FF': 1})"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = my_data.split(split_ratio=0.8)\n",
    "# train_data, valid_data\n",
    "\n",
    "train_iter, valid_iter = torchtext.data.iterator.BucketIterator.splits((train_data, valid_data),\n",
    "                                             batch_sizes=(32, 32),\n",
    "                                             sort_key=lambda x: len(x.text), # field sorted by len\n",
    "                                             sort_within_batch=True,\n",
    "                                             repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<torchtext.data.iterator.BucketIterator object at 0x294821b50>\nbatch:\n \n[torchtext.data.batch.Batch of size 32]\n\t[.labels]:[torch.LongTensor of size 32]\n\t[.text]:[torch.LongTensor of size 15x32]\nbatch_text:\n tensor([[   19,  5547, 13388,    33,     2, 19448,   327, 15408,  3864,  6563,\n          3342,     2,  7627,     0,    39, 15388,   385,  3303,     2,  3085,\n          7258,     0, 20170,   840,  3712,     0,    28,     2, 12502,  9566,\n          1610, 19891],\n        [   40,     2,    13,    68,     5,  8020,   242, 11547,   751,     3,\n             2,    11,    95,    10,   433,     2,   932,   118,     8,   599,\n            97,  3921,   551,    24,    28,     6,   258,     2,    13,    95,\n           338,     2],\n        [    4,     2, 10746,     3,    25,    31,   230,   293,   751,   226,\n             2,   208,   583,  4396,    11,    11,    24,   179,     0,    11,\n             3,  2735,    77,  1693,   950,   477,   492,  4082,   123, 15493,\n            28,     2],\n        [   19,     2,  7299,   519,   185,    67,   454,   512,   751,   674,\n          4669,    45,  6632,    32,   569,   302,    80,  1311,     9,  3199,\n          5855,    91,    60,     8,    11,     4,    47,   297,    27,   138,\n           722,  3485],\n        [   34,     3,    10,   188,     8,     2,    13,    28,    14,    20,\n            16,    74,  9118,    56,     0, 12901,    10,     3,    53,  6434,\n            23,  2204,     9,     6,   153,     6,    12,   314,  4113,   176,\n             8,    32],\n        [    4,  3440,   123,     9,   232,   347,    11,     9,    39,    21,\n         11223,    29,     0, 10658, 14358,  7476,    24,    84,  1638,    12,\n           388,    34,  1759,   645,  3712,  1471,   258,  2624,     3,     8,\n             6,    20],\n        [   19,    97,  2798,  2440,    33,    11,    50,    79, 13263,  1503,\n             3,  2508, 11521,     5,    23,    27,   199,    78,  2930,    12,\n            17,   544,     6, 20830,   950,    10,    28,  1184,     2,  7117,\n           430,  3386],\n        [ 2057,  1572,     3,    10,   264,   135,    27,    45,    16,   960,\n             6,     0,  1294,    56,    20,   358,   107,     4,     4,    12,\n          3871,    95,  6253,  5051,     2,     6,    12,  2177,     0,    20,\n            17,    31],\n        [   10,     3,  1414,     9,     8,     9,     8,   250,   175,  1123,\n           739,     8, 17866,  1768,    14,     5,   420,   351,  4468,     2,\n            10,  2847,     2,    12,     2,  2277,    70,     4,    13,    54,\n             3, 12015],\n        [   19,     6,     0,    20,    15,   788,   290,     8,    58,    13,\n            10,  4797,  7627,     5,    13,   238,   857,   428,     4, 13072,\n            23,     4,    22,    12,    15,    35,    25,  7464,  2774, 10437,\n            46,     8],\n        [   80,   596,  2058,     0,    90,     3,    20,   551,    27,   282,\n            11,    10,    10,    56,   269,    10,    34,     2,   824,     0,\n           796,     0,    11,    12,   344,  2773,   141,  9027,     4,  4042,\n            69,    26],\n        [   56,     5,     2,     2,     5,    20,     5,     8,    49,   195,\n          2118,    15,  1386,    67,  1773,   847,     2,  1632,  1938,    66,\n          2063,     6,   895,   293, 19792,     5,   549,    10,   304,  9489,\n            22,     0],\n        [    2,   100,   184,     2,    67,    12,  6332,  8024,     3,  2180,\n             0,  1186,     0,    13,     3,    35,     2,   726,     5,    28,\n            12,     0,     3,     2,    14,     2,    12,    14,   387,     2,\n            22,    14],\n        [    2,    14,     0,  1602,  1767,    12,   726,     5,     2,     2,\n            14,    30,     2,   634,    11,    20,    14,    14,    28, 11189,\n             2,  1586,   516,     2,     7,     7,     2,     7,     7,     2,\n             2,     7],\n        [    7,     7,     7,     7,     5,    12,     2,     2,     7,   380,\n             7,     5,  7498,     2,  1277,     7,     7,     7,     7,     7,\n             2,     2, 15092,     2,     1,     1,     1,     1,     1,     1,\n             1,     1]])\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'label'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-bff483a73a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_text:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_label:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(train_iter)\n",
    "print('batch:\\n', batch)\n",
    "print('batch_text:\\n', batch.text)\n",
    "print('batch_label:\\n', batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_sequence(seq, to_ix):\n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# training_data = [\n",
    "#     (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "#     (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "# ]\n",
    "# word_to_ix = {}\n",
    "# for sent, tags in training_data:\n",
    "#     for word in sent:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "# print(word_to_ix)\n",
    "# tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# # These will usually be more like 32 or 64 dimensional.\n",
    "# # We will keep them small, so we can see how the weights change as we train.\n",
    "# EMBEDDING_DIM = 6\n",
    "# HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = 2\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    #                   collate_fn=generate_batch)\n",
    "    # for i, (text, offsets, cls) in enumerate(data):\n",
    "    #     optimizer.zero_grad()\n",
    "    #     text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "    #     output = model(text, offsets)\n",
    "    #     loss = criterion(output, cls)\n",
    "    #     train_loss += loss.item()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     train_acc += (output.argmax(1) == cls).sum().item()\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()    \n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}