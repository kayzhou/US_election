{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前不依赖于原有项目，独立完成推特用户意见分类器\n",
    "\n",
    "# 创造训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "data/tokens.train/tokens.08.txt\ndata/tokens.train/tokens.05.txt\ndata/tokens.train/tokens.04.txt\ndata/tokens.train/tokens.06.txt\ndata/tokens.train/tokens.07.txt\n"
    }
   ],
   "source": [
    "# 聚合所有数据\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for in_file in glob.glob(\"data/tokens.train/*.txt\"):\n",
    "    print(in_file)\n",
    "    for line in open(in_file):\n",
    "        label, sentense = line.strip().split(\"\\t\")\n",
    "        if label == \"0\":\n",
    "            label = \"FF\"\n",
    "        elif label == \"1\":\n",
    "            label = \"MP\"\n",
    "        else:\n",
    "            assert \"Error\"\n",
    "        X.append(sentense)\n",
    "        y.append(label)\n",
    "\n",
    "data = {}\n",
    "data[\"label\"] = y\n",
    "data[\"text\"] = X\n",
    "pd.DataFrame(data).to_csv(\"data/train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "TEXT = torchtext.data.Field()\n",
    "LABEL = torchtext.data.LabelField()\n",
    "\n",
    "my_data = torchtext.data.TabularDataset(path='data/train.csv',\n",
    "                                        format='csv', \n",
    "                                        fields=[('labels', LABEL), ('text', TEXT)])\n",
    "\n",
    "# train_data, valid_data = my_data.split(split_ratio=0.8)\n",
    "# train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(my_data, min_freq=10)\n",
    "LABEL.build_vocab(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21142, 3)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(TEXT.vocab), len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "defaultdict(None, {'MP': 0, 'FF': 1, 'label': 2})"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = my_data.split(split_ratio=0.8)\n",
    "# train_data, valid_data\n",
    "\n",
    "train_iter, valid_iter = torchtext.data.iterator.BucketIterator.splits((train_data, valid_data),\n",
    "                                             batch_sizes=(32, 32),\n",
    "                                             sort_key=lambda x: len(x.text), # field sorted by len\n",
    "                                             sort_within_batch=True,\n",
    "                                             repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<torchtext.data.iterator.BucketIterator object at 0x13e76dc90>\nbatch:\n \n[torchtext.data.batch.Batch of size 32]\n\t[.labels]:[torch.LongTensor of size 32]\n\t[.text]:[torch.LongTensor of size 21x32]\nbatch_text:\n tensor([[    0,    13,     0,     2,   789,   253,   547,  1019,   128,     2,\n           303,  4281,     2,     2,   381,  9177,    20,    11,   151,  7175,\n             2,     2,    83,    39,     0, 13573,    11,    52,     2,     6,\n          1944,   489],\n        [    0,   297,   298,     6,     8,  1988,   115,    25,    29,    39,\n             2,     0,    34, 11142,   108,   268,     4,   931,  1118,     0,\n            20,    20,    17,   989,    40,  6441,   121,   169,    20,   809,\n            28,    13],\n        [    0,  1599,   298,   421,    41,     3,  9113,   492,  5066,  1100,\n            18,   961,    13,    20,    10,     0,   102, 13465,    13,     9,\n             0,    17,    11,    13,     0,    92,    21,  5832,    21,     3,\n             2,    11],\n        [    0,     4,    10,     8, 18508,     8,     4, 11322,    12,     9,\n           211,    17,    53,     4,    36,     0,     4,    42,  1274,    16,\n             0,  2163,   110,     0,     9,  1390,  4360,    16,   568,    84,\n             2,   108],\n        [   40,    19,    11,    51,    16,     2,   345,    27,    12,    20,\n            23,     6,   267,   129,  1333,   104,   510,   284,     8,   340,\n            11,   144,    32,  5793,   529,    58,     3,  6341,  8956,    78,\n            15,     8],\n        [   70,     6,   353,   216,  1310,    17,   569,    67,    38,    61,\n          1413,   354,   169,    11,  1762,     2,     4,    31,    74,   275,\n          1585,   104,  1549,    19,    51,    25,    53,    27,    10,   548,\n            45,     2],\n        [  320,   258,     9,     6,     9,     6,     3, 17965,   798,    41,\n         12902,   595,   675,   353,    13,  5836,    43,    49,    11,    44,\n             3,    11,     9,     0,  6856, 12125,    49,    49,   488,     3,\n          4821,     4],\n        [   14,     3,  2552,   365,     6,  1921,     6,   129,    10,   844,\n            18,     8,    25,     0,   197,    10,    14,     3,     0,    36,\n           291,  7769,  9002,  1083,     0,     3,  1268,    14,    17, 13755,\n          1212,     6],\n        [  292,    29,    22,     3,   431,   152,   241,    25,    38, 11439,\n            32,    11,   174,    16,    29,  2101,    83,     2,    10,    83,\n            13,    35,    12,    23,    13,  3440,     4,  8830,    11,    11,\n            14,   345],\n        [    5,   892,   148,   868,     2,     3,     4,   524,  1463,   483,\n             6,    67,   566,   184,   887,    13,    52,    10,    11,  1449,\n           383,   367,    17,     2,     6,    13,     6,    11, 17290,   339,\n           193,   505],\n        [   44,     3,   171,    25,  1379,  3708,    19,   680,    34,   737,\n           264,    63,   212,  3242, 13444,  4753,   273,    21,     2,     9,\n             4,     4,   169,    12,   222, 10758,   413,     2,     4,     3,\n           124,    46],\n        [  230,    43,   389,   189,  1676,     8,   402,   378, 11333,     3,\n             3,  6223,     0,    32,    33,    21,    12,   309,    17,   440,\n           233,    10,   827,    12,     4,    51,    43,    24,    44,  6463,\n            30,    78],\n        [    4,   188,     3,  1572,  5740,    51,    32,     5,     4,   184,\n            35,  3137,    13,    56,   421,  4591,    12,    25,     8,    19,\n             8,  4453,    39,    16,    44,     0,    74, 14939,    13,    13,\n          1171,     3],\n        [   90,     9,     6,  2737,    25,    60,    18,    21,  3556,    13,\n          3023,    64,    53,     8,    19, 15190,    12,   259,  1674,    29,\n            13,   302,   496,  1073,   273,     3,    16,    42,    53,    25,\n             8,    71],\n        [   42,    66,  3229,    24,   650,     9,    16,  7906,     4,   814,\n            11,  1173,  1156,     0,    11,   398,    33, 15628,  4737,  3754,\n          8327,     4,    23,   104,    19,   325,   234,  8950,   799,  6650,\n            99,  4112],\n        [  496,  2292,   632,  1659,    24,  2189,  1867,   108,     0,     9,\n           259,   278,     0,     9,   185,   137,  8452,    23,    16,     4,\n            21,    16,   247,    29,    40,    71,  7397,   585,    16,   854,\n            12,    29],\n        [ 1433,  3424,     2,     9,  8512,  3171,  1716,     9,     8,   231,\n           637,    33,     9,   231,     3,  2043,  9593,    11,  3157,  2147,\n          8182,  1376,    68,     0,  1155,    10,     2,    19,   221, 17683,\n             5,    18],\n        [  540, 10264,   134,    43,     6,  6362,  1179,   108,    56,   546,\n             3,   496,     6,  5607,    85,    13,    12,  6092,     3,    87,\n            90, 18496,     5,   998,   242,     0,     2,  1639,    52,     0,\n           566,  9060],\n        [    5,     2,  3062,    14,    14,    14,    14,     5,    25,    14,\n            43,     5,   420,    14,    22,    14,     2,     0,    16,    12,\n            14,    14,   249,    14,     0,    14,    14,    14,   473,   195,\n           342,    14],\n        [11851,     2,    14,     7,     7,     7,     7,     2,     2,     7,\n             7,     2,     5,     7,     2,     7,     7,     7,  1185,     2,\n             7,     7,     7,     7,     2,     7,     7,     7,     5, 18866,\n             5,     7],\n        [    2,     7,     7,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1]])\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'label'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bff483a73a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_text:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_label:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(train_iter)\n",
    "print('batch:\\n', batch)\n",
    "print('batch_text:\\n', batch.text)\n",
    "print('batch_label:\\n', batch.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# raw word to ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_sequence(seq, to_ix):\n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# training_data = [\n",
    "#     (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "#     (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "# ]\n",
    "# word_to_ix = {}\n",
    "# for sent, tags in training_data:\n",
    "#     for word in sent:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "# print(word_to_ix)\n",
    "# tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# # These will usually be more like 32 or 64 dimensional.\n",
    "# # We will keep them small, so we can see how the weights change as we train.\n",
    "# EMBEDDING_DIM = 6\n",
    "# HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBED_DIM = 300\n",
    "NUN_CLASS = 2\n",
    "model = TweetClassifier(VOCAB_SIZE, EMBED_DIM, NUN_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    #                   collate_fn=generate_batch)\n",
    "    # for i, (text, offsets, cls) in enumerate(data):\n",
    "    #     optimizer.zero_grad()\n",
    "    #     text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "    #     output = model(text, offsets)\n",
    "    #     loss = criterion(output, cls)\n",
    "    #     train_loss += loss.item()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     train_acc += (output.argmax(1) == cls).sum().item()\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()    \n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}